%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Ramaseshan Ramachandran at 2025-01-22 22:50:43 +0530 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}
@comment{jabref-meta: grouping: 0 AllEntriesGroup:; 1 StaticGroup:Markings\;2\;1\;\;\;\;; 2 StaticGroup:ramaseshan:6\;2\;1\;\;\;\;; }



@url{CourseMMML,
	date-added = {2025-01-22 22:50:10 +0530},
	date-modified = {2025-01-22 22:50:37 +0530},
	description = {Multimodal Machine Learning 11-777 - at CMU},
	title = {MMML},
	url = {https://cmu-mmml.github.io/spring2023/},
	viewport = {width=device-width, initial-scale=1}}

@inproceedings{10.1007/978-3-642-15561-1_2,
	abstract = {Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
	address = {Berlin, Heidelberg},
	author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
	booktitle = {Computer Vision -- ECCV 2010},
	date-added = {2025-01-22 22:46:47 +0530},
	date-modified = {2025-01-22 22:46:47 +0530},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	isbn = {978-3-642-15561-1},
	pages = {15--29},
	publisher = {Springer Berlin Heidelberg},
	title = {Every Picture Tells a Story: Generating Sentences from Images},
	year = {2010}}

@article{DBLP:journals/corr/abs-2004-10151,
	abstract = {Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful.
Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.},
	author = {Yonatan Bisk and Ari Holtzman and Jesse Thomason and Jacob Andreas and Yoshua Bengio and Joyce Chai and Mirella Lapata and Angeliki Lazaridou and Jonathan May and Aleksandr Nisnevich and Nicolas Pinto and Joseph P. Turian},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2004-10151.bib},
	date-added = {2025-01-22 22:43:15 +0530},
	date-modified = {2025-01-22 22:43:33 +0530},
	eprint = {2004.10151},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 28 Apr 2020 16:10:02 +0200},
	title = {Experience Grounds Language},
	url = {https://arxiv.org/abs/2004.10151},
	volume = {abs/2004.10151},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2004.10151}}

@article{DBLP:journals/corr/abs-1206-5538,
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	author = {Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1206-5538.bib},
	date-added = {2025-01-22 22:40:48 +0530},
	date-modified = {2025-01-22 22:41:14 +0530},
	eprint = {1206.5538},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
	title = {Unsupervised Feature Learning and Deep Learning: {A} Review and New Perspectives},
	url = {http://arxiv.org/abs/1206.5538},
	volume = {abs/1206.5538},
	year = {2012},
	bdsk-url-1 = {http://arxiv.org/abs/1206.5538}}

@misc{multimodalmachinelearningsurvey,
	abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
	archiveprefix = {arXiv},
	author = {Tadas Baltru{\v s}aitis and Chaitanya Ahuja and Louis-Philippe Morency},
	date-added = {2025-01-22 22:37:49 +0530},
	date-modified = {2025-01-22 22:38:09 +0530},
	eprint = {1705.09406},
	primaryclass = {cs.LG},
	title = {Multimodal Machine Learning: A Survey and Taxonomy},
	url = {https://arxiv.org/abs/1705.09406},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1705.09406}}

@misc{fisher2025bayesianneuralnetworksmake,
	abstract = {Bayesian inference promises a framework for principled uncertainty quantification of neural network predictions. Barriers to adoption include the difficulty of fully characterizing posterior distributions on network parameters and the interpretability of posterior predictive distributions. We demonstrate that under a discretized prior for the inner layer weights, we can exactly characterize the posterior predictive distribution as a Gaussian mixture. This setting allows us to define equivalence classes of network parameter values which produce the same likelihood (training error) and to relate the elements of these classes to the network's scaling regime -- defined via ratios of the training sample size, the size of each layer, and the number of final layer parameters. Of particular interest are distinct parameter realizations that map to low training error and yet correspond to distinct modes in the posterior predictive distribution. We identify settings that exhibit such predictive multimodality, and thus provide insight into the accuracy of unimodal posterior approximations. We also characterize the capacity of a model to "learn from data" by evaluating contraction of the posterior predictive in different scaling regimes.},
	archiveprefix = {arXiv},
	author = {Katharine Fisher and Youssef Marzouk},
	date-added = {2025-01-22 21:50:47 +0530},
	date-modified = {2025-01-22 21:51:30 +0530},
	eprint = {2501.11773},
	primaryclass = {stat.ML},
	title = {Can Bayesian Neural Networks Make Confident Predictions?},
	url = {https://arxiv.org/abs/2501.11773},
	year = {2025},
	bdsk-url-1 = {https://arxiv.org/abs/2501.11773}}

@misc{nguyen2025installcontextawareinstructionaltask,
	abstract = {The improved competence of generative models can help building multi-modal virtual assistants that leverage modalities beyond language. By observing humans performing multi-step tasks, one can build assistants that have situational awareness of actions and tasks being performed, enabling them to cater assistance based on this understanding. In this paper, we develop a Context-aware Instructional Task Assistant with Multi-modal Large Language Models (InsTALL) that leverages an online visual stream (e.g. a user's screen share or video recording) and responds in real-time to user queries related to the task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal model on task videos and paired textual data, and 2) automatically extracts task graph from video data and leverages it at training and inference time. We show InsTALL achieves state-of-the-art performance across proposed sub-tasks considered for multimodal activity understanding -- task recognition (TR), action recognition (AR), next action prediction (AP), and plan prediction (PP) -- and outperforms existing baselines on two novel sub-tasks related to automatic error identification.},
	archiveprefix = {arXiv},
	author = {Pha Nguyen and Sailik Sengupta and Girik Malik and Arshit Gupta and Bonan Min},
	date-added = {2025-01-22 21:48:43 +0530},
	date-modified = {2025-01-22 21:49:05 +0530},
	eprint = {2501.12231},
	primaryclass = {cs.CV},
	title = {InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models},
	url = {https://arxiv.org/abs/2501.12231},
	year = {2025},
	bdsk-url-1 = {https://arxiv.org/abs/2501.12231}}

@misc{wang2025internvideo25empoweringvideomllms,
	abstract = {This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMs' ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short & long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLM's innate abilites (focus and memory), providing new insights for future research on video MLLM. Code and models are available at this https URL},
	archiveprefix = {arXiv},
	author = {Yi Wang and Xinhao Li and Ziang Yan and Yinan He and Jiashuo Yu and Xiangyu Zeng and Chenting Wang and Changlian Ma and Haian Huang and Jianfei Gao and Min Dou and Kai Chen and Wenhai Wang and Yu Qiao and Yali Wang and Limin Wang},
	date-added = {2025-01-22 21:46:15 +0530},
	date-modified = {2025-01-22 21:47:26 +0530},
	eprint = {2501.12386},
	primaryclass = {cs.CV},
	title = {InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling},
	url = {https://arxiv.org/abs/2501.12386},
	year = {2025},
	bdsk-url-1 = {https://arxiv.org/abs/2501.12386}}

@inproceedings{10.5555/2999792.2999959,
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	address = {Red Hook, NY, USA},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
	date-added = {2025-01-22 21:17:13 +0530},
	date-modified = {2025-01-22 21:17:13 +0530},
	location = {Lake Tahoe, Nevada},
	numpages = {9},
	pages = {3111--3119},
	publisher = {Curran Associates Inc.},
	series = {NIPS'13},
	title = {Distributed representations of words and phrases and their compositionality},
	year = {2013}}

@article{10.1109/TMM.2022.3160060,
	abstract = {More and more users are getting used to posting images and text on social networks to share their emotions or opinions. Accordingly, multimodal sentiment analysis has become a research topic of increasing interest in recent years. Typically, there exist affective regions that evoke human sentiment in an image, which are usually manifested by corresponding words in people's comments. Similarly, people also tend to portray the affective regions of an image when composing image descriptions. As a result, the relationship between image affective regions and the associated text is of great significance for multimodal sentiment analysis. However, most of the existing multimodal sentiment analysis approaches simply concatenate features from image and text, which could not fully explore the interaction between them, leading to suboptimal results. Motivated by this observation, we propose a new image-text interaction network (ITIN) to investigate the relationship between affective image regions and text for multimodal sentiment analysis. Specifically, we introduce a cross-modal alignment module to capture region-word correspondence, based on which multimodal features are fused through an adaptive cross-modal gating module. Moreover, considering the complementary role of context information on sentiment analysis, we integrate the individual-modal contextual feature representations for achieving more reliable prediction. Extensive experimental results and comparisons on public datasets demonstrate that the proposed model is superior to the state-of-the-art methods.},
	author = {Zhu, Tong and Li, Leida and Yang, Jufeng and Zhao, Sicheng and Liu, Hantao and Qian, Jiansheng},
	date-added = {2025-01-22 20:48:32 +0530},
	date-modified = {2025-01-22 20:48:32 +0530},
	doi = {10.1109/TMM.2022.3160060},
	issn = {1520-9210},
	issue_date = {2023},
	journal = {Trans. Multi.},
	month = jan,
	numpages = {11},
	pages = {3375--3385},
	publisher = {IEEE Press},
	title = {Multimodal Sentiment Analysis With Image-Text Interaction Network},
	url = {https://doi.org/10.1109/TMM.2022.3160060},
	volume = {25},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TMM.2022.3160060}}

@inproceedings{10501825,
	author = {Li, Yuxin and Jiang, Shan and Chaomurilige},
	booktitle = {2023 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)},
	date-added = {2025-01-22 20:46:59 +0530},
	date-modified = {2025-01-22 20:46:59 +0530},
	doi = {10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics60724.2023.00067},
	keywords = {Sentiment analysis;Analytical models;Visualization;Correlation;Image recognition;Text recognition;Transforms;multimodal emotion recognition;soft attention;information fusion;Internet of Things (IoT);Social Internet of Things (SIoT)},
	pages = {281-286},
	title = {Multimodal Sentiment Analysis With Image-Text Correlation Modal},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics60724.2023.00067}}

@article{10.1145/3545572,
	abstract = {Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This article focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, physiological signals, flow, RGB, pose, depth, mesh, and point cloud. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the past five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Last, main issues are highlighted separately for each domain, along with their possible future research directions.},
	address = {New York, NY, USA},
	articleno = {76},
	author = {Jabeen, Summaira and Li, Xi and Amin, Muhammad Shoib and Bourahla, Omar and Li, Songyuan and Jabbar, Abdul},
	date-added = {2025-01-22 20:44:51 +0530},
	date-modified = {2025-01-22 20:44:51 +0530},
	doi = {10.1145/3545572},
	issn = {1551-6857},
	issue_date = {April 2023},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	keywords = {Deep learning, multimedia, multimodal learning, datasets, neural networks, survey},
	month = feb,
	number = {2s},
	numpages = {41},
	publisher = {Association for Computing Machinery},
	title = {A Review on Methods and Applications in Multimodal Deep Learning},
	url = {https://doi.org/10.1145/3545572},
	volume = {19},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3545572}}

@misc{summaira2022reviewmethodsapplicationsmultimodal,
	archiveprefix = {arXiv},
	author = {Jabeen Summaira and Xi Li and Amin Muhammad Shoib and Jabbar Abdul},
	date-added = {2025-01-22 20:43:22 +0530},
	date-modified = {2025-01-22 20:43:22 +0530},
	eprint = {2202.09195},
	primaryclass = {cs.LG},
	title = {A Review on Methods and Applications in Multimodal Deep Learning},
	url = {https://arxiv.org/abs/2202.09195},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2202.09195}}

@article{Dipaola:2023aa,
	abstract = {Predicting clinical deterioration in COVID-19 patients remains a challenging task in the Emergency Department (ED). To address this aim, we developed an artificial neural network using textual (e.g. patient history) and tabular (e.g. laboratory values) data from ED electronic medical reports. The predicted outcomes were 30-day mortality and ICU admission. We included consecutive patients from Humanitas Research Hospital and San Raffaele Hospital in the Milan area between February 20 and May 5, 2020. We included 1296 COVID-19 patients. Textual predictors consisted of patient history, physical exam, and radiological reports. Tabular predictors included age, creatinine, C-reactive protein, hemoglobin, and platelet count. TensorFlow tabular-textual model performance indices were compared to those of models implementing only tabular data. For 30-day mortality, the combined model yielded slightly better performances than the tabular fastai and XGBoost models, with AUC 0.87 $\pm$0.02, F1 score 0.62 $\pm$0.10 and an MCC 0.52 $\pm$0.04 (p < 0.32). As for ICU admission, the combined model MCC was superior (p < 0.024) to the tabular models. Our results suggest that a combined textual and tabular model can effectively predict COVID-19 prognosis which may assist ED physicians in their decision-making process.},
	author = {Dipaola, Franca and Gatti, Mauro and Giaj Levra, Alessandro and Men{\`e}, Roberto and Shiffer, Dana and Faccincani, Roberto and Raouf, Zainab and Secchi, Antonio and Rovere Querini, Patrizia and Voza, Antonio and Badalamenti, Salvatore and Solbiati, Monica and Costantino, Giorgio and Savevski, Victor and Furlan, Raffaello},
	date = {2023/07/05},
	date-added = {2025-01-22 20:41:55 +0530},
	date-modified = {2025-01-22 20:41:55 +0530},
	doi = {10.1038/s41598-023-37512-3},
	id = {Dipaola2023},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {10868},
	title = {Multimodal deep learning for COVID-19 prognosis prediction in the emergency department: a bi-centric study},
	url = {https://doi.org/10.1038/s41598-023-37512-3},
	volume = {13},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-023-37512-3}}

@article{Dang_2020,
	author = {Dang, Nhan Cach and Moreno-Garc{\'\i}a, Mar{\'\i}a N. and De la Prieta, Fernando},
	date-added = {2025-01-15 08:41:39 +0530},
	date-modified = {2025-01-15 08:41:39 +0530},
	doi = {10.3390/electronics9030483},
	issn = {2079-9292},
	journal = {Electronics},
	month = mar,
	number = {3},
	pages = {483},
	publisher = {MDPI AG},
	title = {Sentiment Analysis Based on Deep Learning: A Comparative Study},
	url = {http://dx.doi.org/10.3390/electronics9030483},
	volume = {9},
	year = {2020},
	bdsk-url-1 = {http://dx.doi.org/10.3390/electronics9030483}}

@inproceedings{socher-etal-2013-recursive,
	address = {Seattle, Washington, USA},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2025-01-15 08:36:15 +0530},
	date-modified = {2025-01-15 08:36:15 +0530},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	month = oct,
	pages = {1631--1642},
	publisher = {Association for Computational Linguistics},
	title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
	url = {https://aclanthology.org/D13-1170/},
	year = {2013},
	bdsk-url-1 = {https://aclanthology.org/D13-1170/}}

@article{johnson-etal-2017-googles,
	abstract = {We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT{'}14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT{'}14 and WMT{'}15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.},
	address = {Cambridge, MA},
	author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	date-added = {2024-11-12 04:44:56 +0530},
	date-modified = {2024-11-12 04:44:56 +0530},
	doi = {10.1162/tacl_a_00065},
	editor = {Lee, Lillian and Johnson, Mark and Toutanova, Kristina},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {339--351},
	publisher = {MIT Press},
	title = {{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	url = {https://aclanthology.org/Q17-1024},
	volume = {5},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/Q17-1024},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00065}}

@book{jurafsky_speech_2024,
	author = {Jurafsky, Daniel and Martin, James H.},
	date-added = {2024-11-06 18:37:14 +0530},
	date-modified = {2024-11-06 18:37:14 +0530},
	edition = {Third Edition draft},
	title = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition}},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	year = {2024},
	bdsk-url-1 = {https://web.stanford.edu/~jurafsky/slp3/}}

@webpage{10.5555/3042817.3043064,
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	date-added = {2024-11-05 07:16:17 +0530},
	date-modified = {2024-11-05 07:20:05 +0530},
	location = {Atlanta, GA, USA},
	pages = {III--1139--III--1147},
	publisher = {JMLR.org},
	series = {ICML'13},
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://www.cs.toronto.edu/~fritz/absps/momentum.pdf},
	year = {2013},
	bdsk-url-1 = {https://www.cs.toronto.edu/~fritz/absps/momentum.pdf}}

@inproceedings{pmlr-v28-sutskever13,
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   },
	address = {Atlanta, Georgia, USA},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	date-added = {2024-11-05 07:12:58 +0530},
	date-modified = {2024-11-05 07:12:58 +0530},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = {17--19 Jun},
	number = {3},
	pages = {1139--1147},
	pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://proceedings.mlr.press/v28/sutskever13.html},
	volume = {28},
	year = {2013},
	bdsk-url-1 = {https://proceedings.mlr.press/v28/sutskever13.html}}

@webpage{818041,
	author = {Gers, F.A. and Schmidhuber, J. and Cummins, F.},
	booktitle = {1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)},
	date-added = {2024-11-05 06:22:59 +0530},
	date-modified = {2024-11-05 07:06:00 +0530},
	doi = {10.1049/cp:19991218},
	pages = {850-855 vol.2},
	title = {Learning to forget: continual prediction with LSTM},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10f98b86797ebf6c8caea6f54cacbc5a50e8b34},
	volume = {2},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1049/cp:19991218},
	bdsk-url-2 = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10f98b86797ebf6c8caea6f54cacbc5a50e8b34}}

@inproceedings{cho-etal-2014-learning,
	address = {Doha, Qatar},
	author = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	date-added = {2024-10-15 15:29:53 +0530},
	date-modified = {2024-10-15 15:29:53 +0530},
	doi = {10.3115/v1/D14-1179},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	month = oct,
	pages = {1724--1734},
	publisher = {Association for Computational Linguistics},
	title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
	url = {https://aclanthology.org/D14-1179},
	year = {2014},
	bdsk-url-1 = {https://aclanthology.org/D14-1179},
	bdsk-url-2 = {https://doi.org/10.3115/v1/D14-1179}}

@inproceedings{cho-etal-2014-properties,
	address = {Doha, Qatar},
	author = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
	date-added = {2024-10-15 11:44:53 +0530},
	date-modified = {2024-10-15 11:44:53 +0530},
	doi = {10.3115/v1/W14-4012},
	editor = {Wu, Dekai and Carpuat, Marine and Carreras, Xavier and Vecchi, Eva Maria},
	month = oct,
	pages = {103--111},
	publisher = {Association for Computational Linguistics},
	title = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
	url = {https://aclanthology.org/W14-4012},
	year = {2014},
	bdsk-url-1 = {https://aclanthology.org/W14-4012},
	bdsk-url-2 = {https://doi.org/10.3115/v1/W14-4012}}

@misc{chung2014empiricalevaluationgatedrecurrent,
	archiveprefix = {arXiv},
	author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
	date-added = {2024-10-15 11:42:06 +0530},
	date-modified = {2024-10-15 11:42:06 +0530},
	eprint = {1412.3555},
	primaryclass = {cs.NE},
	title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	url = {https://arxiv.org/abs/1412.3555},
	year = {2014},
	bdsk-url-1 = {https://arxiv.org/abs/1412.3555}}

@article{10.5555/177910.177914,
	address = {USA},
	author = {Gage, Philip},
	date-added = {2024-09-27 07:00:20 +0530},
	date-modified = {2024-09-27 07:00:20 +0530},
	issn = {0898-9788},
	issue_date = {Feb. 1994},
	journal = {C Users J.},
	month = feb,
	number = {2},
	numpages = {16},
	pages = {23--38},
	publisher = {R \& D Publications, Inc.},
	title = {A new algorithm for data compression},
	volume = {12},
	year = {1994}}

@webpage{zouhar2024formalperspectivebytepairencoding,
	abstract = {Byte-Pair Encoding (BPE) is a popular algo-
rithm used for tokenizing data in NLP, de-
spite being devised initially as a compression
method. BPE appears to be a greedy algorithm
at face value, but the underlying optimization
problem that BPE seeks to solve has not yet
been laid down. We formalize BPE as a combi-
natorial optimization problem. Via submodular
functions, we prove that the iterative greedy
version is a 1
σ(µ⋆ ) (1−e−σ(µ⋆ ))-approximation
of an optimal merge sequence, where σ(µ⋆) is
the total backward curvature with respect to the
optimal merge sequence µ⋆. Empirically the
lower bound of the approximation is ≈0.37.
We provide a faster implementation of BPE
which improves the runtime complexity from
O(NM) to O(Nlog M), where N is the se-
quence length and M is the merge count. Fi-
nally, we optimize the brute-force algorithm for
optimal BPE using memoization.},
	archiveprefix = {arXiv},
	author = {Vil{\'e}m Zouhar and Clara Meister and Juan Luis Gastaldi and Li Du and Tim Vieira and Mrinmaya Sachan and Ryan Cotterell},
	date-added = {2024-09-27 06:52:11 +0530},
	date-modified = {2025-01-22 21:12:21 +0530},
	eprint = {2306.16837},
	primaryclass = {cs.CL},
	title = {A Formal Perspective on Byte-Pair Encoding},
	url = {https://arxiv.org/pdf/2306.16837},
	year = {2024},
	bdsk-url-1 = {https://arxiv.org/abs/2306.16837},
	bdsk-url-2 = {https://arxiv.org/pdf/2306.16837}}

@article{rumelhart1986learning,
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	date-added = {2024-09-25 18:51:11 +0530},
	date-modified = {2024-09-25 18:51:11 +0530},
	journal = {nature},
	number = {6088},
	pages = {533--536},
	publisher = {Nature Publishing Group UK London},
	title = {Learning representations by back-propagating errors},
	volume = {323},
	year = {1986}}

@webpage{mikolov-etal-2013-linguistic,
	address = {Atlanta, Georgia},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	booktitle = {Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2024-09-25 17:40:51 +0530},
	date-modified = {2024-09-25 17:41:22 +0530},
	editor = {Vanderwende, Lucy and Daum{\'e} III, Hal and Kirchhoff, Katrin},
	lastchecked = {https://aclanthology.org/N13-1090.pdf},
	month = jun,
	pages = {746--751},
	publisher = {Association for Computational Linguistics},
	title = {Linguistic Regularities in Continuous Space Word Representations},
	url = {https://aclanthology.org/N13-1090},
	year = {2013},
	bdsk-url-1 = {https://aclanthology.org/N13-1090}}

@book{10.5555/1162264,
	address = {Berlin, Heidelberg},
	author = {Bishop, Christopher M.},
	date-added = {2024-09-19 08:35:35 +0530},
	date-modified = {2024-09-19 08:35:35 +0530},
	isbn = {0387310738},
	publisher = {Springer-Verlag},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006}}

@book{Deisenroth_Faisal_Ong_2020,
	author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
	date-added = {2024-09-19 04:59:55 +0530},
	date-modified = {2024-09-19 04:59:55 +0530},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Mathematics for Machine Learning},
	year = {2020}}

@inproceedings{pmlr-vR5-morin05a,
	author = {Morin, Frederic and Bengio, Yoshua},
	booktitle = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
	date-added = {2024-09-09 13:24:17 +0530},
	date-modified = {2024-09-09 13:24:17 +0530},
	editor = {Cowell, Robert G. and Ghahramani, Zoubin},
	month = {06--08 Jan},
	note = {Reissued by PMLR on 30 March 2021.},
	pages = {246--252},
	pdf = {http://proceedings.mlr.press/r5/morin05a/morin05a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Hierarchical Probabilistic Neural Network Language Model},
	url = {https://proceedings.mlr.press/r5/morin05a.html},
	volume = {R5},
	year = {2005},
	bdsk-url-1 = {https://proceedings.mlr.press/r5/morin05a.html}}

@article{arora-etal-2016-latent,
	abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.},
	address = {Cambridge, MA},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	date-added = {2024-09-09 13:11:53 +0530},
	date-modified = {2024-09-09 13:11:53 +0530},
	doi = {10.1162/tacl_a_00106},
	editor = {Lee, Lillian and Johnson, Mark and Toutanova, Kristina},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {385--399},
	publisher = {MIT Press},
	title = {A Latent Variable Model Approach to {PMI}-based Word Embeddings},
	url = {https://aclanthology.org/Q16-1028},
	volume = {4},
	year = {2016},
	bdsk-url-1 = {https://aclanthology.org/Q16-1028},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00106}}

@misc{behdin2023sparsegaussiangraphicalmodels,
	archiveprefix = {arXiv},
	author = {Kayhan Behdin and Wenyu Chen and Rahul Mazumder},
	date-added = {2024-09-09 05:57:55 +0530},
	date-modified = {2024-09-09 05:57:55 +0530},
	eprint = {2307.09366},
	primaryclass = {cs.LG},
	title = {Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives},
	url = {https://arxiv.org/abs/2307.09366},
	year = {2023},
	bdsk-url-1 = {https://arxiv.org/abs/2307.09366}}

@misc{alshbanat2024surveylatentfactormodels,
	archiveprefix = {arXiv},
	author = {Hind I. Alshbanat and Hafida Benhidour and Said Kerrache},
	date-added = {2024-09-08 11:03:05 +0530},
	date-modified = {2024-09-08 11:03:05 +0530},
	eprint = {2405.18068},
	primaryclass = {cs.IR},
	title = {A Survey of Latent Factor Models in Recommender Systems},
	url = {https://arxiv.org/abs/2405.18068},
	year = {2024},
	bdsk-url-1 = {https://arxiv.org/abs/2405.18068}}

@book{Maths4ML,
	author = {Marc Peter Deisenroth, A. Aldo Faisal and Cheng Soon Ong},
	date-added = {2024-09-07 21:10:32 +0530},
	date-modified = {2024-09-07 21:14:11 +0530},
	publisher = {Cambridge University Press},
	title = {Mathematics for Machine Learning},
	url = {https://mml-book.github.io/book/mml-book.pdf},
	bdsk-url-1 = {https://mml-book.github.io/book/mml-book.pdf}}

@url{PCATutorialGoodOne,
	author = {Jon Shlens},
	date-added = {2024-09-07 19:24:32 +0530},
	date-modified = {2024-09-07 19:31:31 +0530},
	title = {A TUTORIAL ON PRINCIPAL COMPONENT ANALYSIS Derivation, Discussion and Singular Value Decomposition},
	bdsk-url-1 = {https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf}}

@webpage{nakov-hearst-2008-solving,
	address = {Columbus, Ohio},
	author = {Nakov, Preslav and Hearst, Marti A.},
	booktitle = {Proceedings of ACL-08: HLT},
	date-added = {2024-08-28 12:53:30 +0530},
	date-modified = {2024-08-28 12:53:39 +0530},
	editor = {Moore, Johanna D. and Teufel, Simone and Allan, James and Furui, Sadaoki},
	month = jun,
	pages = {452--460},
	publisher = {Association for Computational Linguistics},
	title = {Solving Relational Similarity Problems Using the Web as a Corpus},
	url = {https://aclanthology.org/P08-1052},
	year = {2008},
	bdsk-url-1 = {https://aclanthology.org/P08-1052}}

@webpage{Turney_2010,
	author = {Turney, P. D. and Pantel, P.},
	date-added = {2024-08-28 11:47:07 +0530},
	date-modified = {2024-08-28 11:47:25 +0530},
	doi = {10.1613/jair.2934},
	issn = {1076-9757},
	journal = {Journal of Artificial Intelligence Research},
	month = feb,
	pages = {141--188},
	publisher = {AI Access Foundation},
	title = {From Frequency to Meaning: Vector Space Models of Semantics},
	url = {http://dx.doi.org/10.1613/jair.2934},
	volume = {37},
	year = {2010},
	bdsk-url-1 = {http://dx.doi.org/10.1613/jair.2934}}

@webpage{landauer1997solution,
	author = {Landauer, Thomas K and Dumais, Susan T},
	date-added = {2024-08-28 11:17:35 +0530},
	date-modified = {2024-08-28 11:19:16 +0530},
	journal = {Psychological review},
	number = {2},
	pages = {211},
	publisher = {American Psychological Association},
	title = {A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
	url = {https://www.stat.cmu.edu/~cshalizi/350/2008/readings/Landauer-Dumais.pdf},
	volume = {104},
	year = {1997},
	bdsk-url-1 = {https://www.stat.cmu.edu/~cshalizi/350/2008/readings/Landauer-Dumais.pdf}}

@webpage{10.5555/1861751.1861756,
	abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
	address = {El Segundo, CA, USA},
	author = {Turney, Peter D. and Pantel, Patrick},
	date-added = {2024-08-28 06:46:16 +0530},
	date-modified = {2024-08-28 06:47:32 +0530},
	issn = {1076-9757},
	issue_date = {January 2010},
	journal = {J. Artif. Int. Res.},
	month = {jan},
	number = {1},
	numpages = {48},
	pages = {141--188},
	publisher = {AI Access Foundation},
	title = {From frequency to meaning: vector space models of semantics},
	url = {https://www.jair.org/index.php/jair/article/view/10640/25440},
	volume = {37},
	year = {2010},
	bdsk-url-1 = {https://www.jair.org/index.php/jair/article/view/10640/25440}}

@webpage{levy2015improving,
	author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
	date-added = {2024-08-27 07:09:31 +0530},
	date-modified = {2024-08-28 06:39:58 +0530},
	journal = {Transactions of the association for computational linguistics},
	pages = {211--225},
	publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~{\ldots}},
	rating = {5},
	read = {1},
	title = {Improving distributional similarity with lessons learned from word embeddings},
	url = {https://aclanthology.org/Q15-1016.pdf},
	volume = {3},
	year = {2015},
	bdsk-url-1 = {https://aclanthology.org/Q15-1016.pdf},
	bdsk-url-2 = {https://aclanthology.org/Q15-1016.pdf}}

@article{DBLP:journals/corr/AroraLLMR15,
	author = {Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/AroraLLMR15.bib},
	date-added = {2024-08-26 07:20:38 +0530},
	date-modified = {2024-08-26 07:20:38 +0530},
	eprint = {1502.03520},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Sun, 08 Aug 2021 16:40:51 +0200},
	title = {Random Walks on Context Spaces: Towards an Explanation of the Mysteries of Semantic Word Embeddings},
	url = {http://arxiv.org/abs/1502.03520},
	volume = {abs/1502.03520},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1502.03520}}

@misc{aghajanyan2020intrinsic,
	annote = {Precursor to LoRA},
	archiveprefix = {arXiv},
	author = {Armen Aghajanyan and Luke Zettlemoyer and Sonal Gupta},
	date-added = {2024-03-31 19:45:29 +0530},
	date-modified = {2024-03-31 19:45:34 +0530},
	eprint = {2012.13255},
	primaryclass = {cs.LG},
	title = {Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/pdf/2012.13255.pdf}}

@misc{li2018measuring,
	annote = {Basis for LoRA},
	archiveprefix = {arXiv},
	author = {Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski},
	date-added = {2024-03-31 19:15:07 +0530},
	date-modified = {2024-03-31 19:15:14 +0530},
	eprint = {1804.08838},
	primaryclass = {cs.LG},
	title = {Measuring the Intrinsic Dimension of Objective Landscapes},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/pdf/1804.08838.pdf}}

@article{hu2021lora,
	annote = {Lora, low rank approximation},
	author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	date-added = {2024-03-31 18:33:08 +0530},
	date-modified = {2024-03-31 18:33:14 +0530},
	journal = {arXiv preprint arXiv:2106.09685},
	title = {Lora: Low-rank adaptation of large language models},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/pdf/2106.09685.pdf}}

@article{9416312,
	annote = {knowledge graph, thoery, survey},
	author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
	date-added = {2024-03-21 10:52:55 +0530},
	date-modified = {2024-03-21 10:53:05 +0530},
	doi = {10.1109/TNNLS.2021.3070843},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Cognition;Knowledge based systems;Semantics;Knowledge acquisition;Task analysis;Taxonomy;Extraterrestrial measurements;Deep learning;knowledge graph completion (KGC);knowledge graph;reasoning;relation extraction;representation learning},
	number = {2},
	pages = {494-514},
	title = {A Survey on Knowledge Graphs: Representation, Acquisition, and Applications},
	volume = {33},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TNNLS.2021.3070843},
	bdsk-url-2 = {https://arxiv.org/pdf/2002.00388.pdf%E2%80%8Barxiv.org%C3%82%C2%A0}}

@article{Swanson:1988aa,
	address = {Graduate Library School, University of Chicago, Illinois 60637.},
	author = {Swanson, D R},
	crdt = {1988/01/01 00:00},
	date = {1988 Summer},
	date-added = {2024-03-20 08:34:00 +0530},
	date-modified = {2024-03-20 08:34:00 +0530},
	dcom = {19890825},
	doi = {10.1353/pbm.1988.0009},
	edat = {1988/01/01 00:00},
	issn = {0031-5982 (Print); 0031-5982 (Linking)},
	jid = {0401132},
	journal = {Perspect Biol Med},
	jt = {Perspectives in biology and medicine},
	language = {eng},
	lr = {20190912},
	mh = {Humans; Magnesium Deficiency/*complications; Migraine Disorders/*etiology/physiopathology/psychology},
	mhda = {1988/01/01 00:01},
	month = {Summer},
	number = {4},
	own = {NLM},
	pages = {526--557},
	phst = {1988/01/01 00:00 {$[$}pubmed{$]$}; 1988/01/01 00:01 {$[$}medline{$]$}; 1988/01/01 00:00 {$[$}entrez{$]$}},
	pl = {United States},
	pmid = {3075738},
	pst = {ppublish},
	pt = {Journal Article; Review},
	rf = {158},
	sb = {IM},
	status = {MEDLINE},
	title = {Migraine and magnesium: eleven neglected connections.},
	volume = {31},
	year = {1988},
	bdsk-url-1 = {https://doi.org/10.1353/pbm.1988.0009}}

@webpage{rohde2006improved,
	abstract = {The lexical semantic system is an important compo-
nent of human language and cognitive processing. One
approach to modeling semantic knowledge makes use
of hand-constructed networks or trees of interconnected
word senses (Miller, Beckwith, Fellbaum, Gross, &
Miller, 1990; Jarmasz & Szpakowicz, 2003). An al-
ternative approach seeks to model word meanings as
high-dimensional vectors, which are derived from the co-
occurrence of words in unlabeled text corpora (Landauer
& Dumais, 1997; Burgess & Lund, 1997a). This pa-
per introduces a new vector-space method for deriving
word-meanings from large corpora that was inspired by
the HAL and LSA models, but which achieves better
and more consistent results in predicting human similarity
judgments. We explain the new model, known as COALS,
and how it relates to prior methods, and then evaluate the
various models on a range of tasks, including a novel set
of semantic similarity ratings involving both semantically
and morphologically related terms.},
	author = {Rohde, Douglas LT and Gonnerman, Laura M and Plaut, David C},
	date-added = {2024-03-12 06:11:09 +0530},
	date-modified = {2025-01-22 21:13:45 +0530},
	journal = {Communications of the ACM},
	number = {627-633},
	pages = {116},
	title = {An improved model of semantic similarity based on lexical co-occurrence},
	url = {https://cnbc.cmu.edu/~plaut/papers/pdf/RohdeGonnermanPlautSUB-CogSci.COALS.pdf},
	volume = {8},
	year = {2006},
	bdsk-url-1 = {https://cnbc.cmu.edu/~plaut/papers/pdf/RohdeGonnermanPlautSUB-CogSci.COALS.pdf},
	bdsk-url-2 = {https://cnbc.cmu.edu/~plaut/papers/pdf/RohdeGonnermanPlautSUB-CogSci.COALS.pdf}}

@inproceedings{lowe2001towards,
	author = {Lowe, Will},
	booktitle = {Proceedings of the annual meeting of the cognitive science society},
	date-added = {2024-03-12 05:37:12 +0530},
	date-modified = {2024-03-12 05:37:12 +0530},
	number = {23},
	title = {Towards a theory of semantic space},
	volume = {23},
	year = {2001},
	bdsk-url-1 = {https://dl.conjugateprior.org/preprints/towards-a-theory.pdf}}

@inproceedings{lee-1999-measures,
	address = {College Park, Maryland, USA},
	author = {Lee, Lillian},
	booktitle = {Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2024-03-11 10:08:18 +0530},
	date-modified = {2024-03-11 10:08:18 +0530},
	doi = {10.3115/1034678.1034693},
	month = jun,
	pages = {25--32},
	publisher = {Association for Computational Linguistics},
	title = {Measures of Distributional Similarity},
	url = {https://aclanthology.org/P99-1004},
	year = {1999},
	bdsk-url-1 = {https://aclanthology.org/P99-1004},
	bdsk-url-2 = {https://doi.org/10.3115/1034678.1034693}}

@misc{clark2017simple,
	annote = {Reading Comprehension},
	archiveprefix = {arXiv},
	author = {Christopher Clark and Matt Gardner},
	date-added = {2024-03-06 10:28:27 +0530},
	date-modified = {2024-03-06 10:28:34 +0530},
	eprint = {1710.10723},
	primaryclass = {cs.CL},
	title = {Simple and Effective Multi-Paragraph Reading Comprehension},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1710.10723.pdf}}

@misc{seo2018bidirectional,
	archiveprefix = {arXiv},
	author = {Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},
	date-added = {2024-03-06 10:11:36 +0530},
	date-modified = {2024-03-06 10:12:06 +0530},
	eprint = {https://arxiv.org/abs/1611.01603},
	primaryclass = {cs.CL},
	title = {Bidirectional Attention Flow for Machine Comprehension},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1611.01603},
	bdsk-url-2 = {https://arxiv.org/pdf/1611.01603.pdf}}

@misc{hermann2015teaching,
	archiveprefix = {arXiv},
	author = {Karl Moritz Hermann and Tom{\'a}{\v s} Ko{\v c}isk{\'y} and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},
	date-added = {2024-03-03 10:08:52 +0530},
	date-modified = {2024-03-04 05:17:22 +0530},
	eprint = {https://arxiv.org/pdf/1506.03340.pdf},
	primaryclass = {cs.CL},
	title = {Teaching Machines to Read and Comprehend},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhYYm9va21hcmtfEGIuLi8uLi8uLi9UQS9SZWZlcmVuY2VzL0NvbnZlcnNhdGlvbk1vZGVsaW5nL1RlYWNoaW5nIE1hY2hpbmVzIHRvIFJlYWQgYW5kIENvbXByZWhlbmQxNTA2LjAzMzQwLnBkZk8RBLRib29rtAQAAAAABBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAwAABQAAAAEBAABVc2VycwAAAAMAAAABAQAAcmFtAAkAAAABAQAARG9jdW1lbnRzAAAABgAAAAEBAABHaXRIdWIAAAIAAAABAQAAVEEAAAoAAAABAQAAUmVmZXJlbmNlcwAAFAAAAAEBAABDb252ZXJzYXRpb25Nb2RlbGluZzYAAAABAQAAVGVhY2hpbmcgTWFjaGluZXMgdG8gUmVhZCBhbmQgQ29tcHJlaGVuZDE1MDYuMDMzNDAucGRmAAAgAAAAAQYAAAQAAAAUAAAAIAAAADQAAABEAAAAUAAAAGQAAACAAAAACAAAAAQDAAAdQgAAAAAAAAgAAAAEAwAAqeIGAAAAAAAIAAAABAMAAGcuCQAAAAAACAAAAAQDAADjLwkAAAAAAAgAAAAEAwAAekIKAAAAAAAIAAAABAMAAAJFCgAAAAAACAAAAAQDAAAIRQoAAAAAAAgAAAAEAwAAKEUKAAAAAAAgAAAAAQYAAOgAAAD4AAAACAEAABgBAAAoAQAAOAEAAEgBAABYAQAACAAAAAAEAABBxcqhkkYATBgAAAABAgAAAQAAAAAAAAAPAAAAAAAAAAAAAAAAAAAACAAAAAQDAAAGAAAAAAAAAAQAAAADAwAA9QEAAAgAAAABCQAAZmlsZTovLy8MAAAAAQEAAE1hY2ludG9zaCBIRAgAAAAEAwAAAACHETkAAAAIAAAAAAQAAEHGgh1lgAAAJAAAAAEBAAA5MUQzQ0IwNC04MkFBLTQyRDUtQjNCMy1GRjBDQjU2MkQ5M0QYAAAAAQIAAIEAAAABAAAA7xMAAAEAAAAAAAAAAAAAAAEAAAABAQAALwAAAAAAAAABBQAAJwEAAAECAAA2YmYyYjQzNGZiNjcyZGY1NGM2YmJkNTg2ODk4YjM0ZWI0MDQyNjE5NzhjNmU3MzU2MzI3NGVjYWIxNTFiMTM3OzAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwOzAwMDAwMDAwMDAwMDAwMjA7Y29tLmFwcGxlLmFwcC1zYW5kYm94LnJlYWQtd3JpdGU7MDE7MDEwMDAwMTE7MDAwMDAwMDAwMDBhNDUyODs2MTsvdXNlcnMvcmFtL2RvY3VtZW50cy9naXRodWIvdGEvcmVmZXJlbmNlcy9jb252ZXJzYXRpb25tb2RlbGluZy90ZWFjaGluZyBtYWNoaW5lcyB0byByZWFkIGFuZCBjb21wcmVoZW5kMTUwNi4wMzM0MC5wZGYAAMwAAAD+////AQAAAAAAAAAQAAAABBAAAMAAAAAAAAAABRAAAGgBAAAAAAAAEBAAAKABAAAAAAAAQBAAAJABAAAAAAAAAiAAAGwCAAAAAAAABSAAANwBAAAAAAAAECAAAOwBAAAAAAAAESAAACACAAAAAAAAEiAAAAACAAAAAAAAEyAAABACAAAAAAAAICAAAEwCAAAAAAAAMCAAAHgCAAAAAAAAAcAAAMABAAAAAAAAEcAAABQAAAAAAAAAEsAAANABAAAAAAAAgPAAAIACAAAAAAAAAAgADQAaACMAiAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAVA}}

@misc{chen2017reading,
	archiveprefix = {arXiv},
	author = {Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
	date-added = {2024-02-22 13:15:35 +0530},
	date-modified = {2024-02-22 13:15:35 +0530},
	eprint = {1704.00051},
	primaryclass = {cs.CL},
	title = {Reading Wikipedia to Answer Open-Domain Questions},
	year = {2017}}

@misc{wang2016machine,
	archiveprefix = {arXiv},
	author = {Shuohang Wang and Jing Jiang},
	date-added = {2024-02-22 12:25:01 +0530},
	date-modified = {2024-02-22 12:25:01 +0530},
	eprint = {1608.07905},
	primaryclass = {cs.CL},
	title = {Machine Comprehension Using Match-LSTM and Answer Pointer},
	year = {2016}}

@inproceedings{li-roth-2002-learning,
	author = {Li, Xin and Roth, Dan},
	booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
	date-added = {2024-02-21 10:16:02 +0530},
	date-modified = {2024-02-21 10:16:02 +0530},
	title = {Learning Question Classifiers},
	url = {https://aclanthology.org/C02-1150},
	year = {2002},
	bdsk-url-1 = {https://aclanthology.org/C02-1150}}

@inproceedings{ratnaparkhi-1996-maximum,
	annote = {POS},
	author = {Ratnaparkhi, Adwait},
	booktitle = {Conference on Empirical Methods in Natural Language Processing},
	date-added = {2024-02-09 06:56:10 +0530},
	date-modified = {2024-02-09 06:56:29 +0530},
	title = {A Maximum Entropy Model for Part-Of-Speech Tagging},
	url = {https://aclanthology.org/W96-0213},
	year = {1996},
	bdsk-url-1 = {https://aclanthology.org/W96-0213}}

@misc{xu2023marginalized,
	archiveprefix = {arXiv},
	author = {Xuechun Xu and Joakim Jald{\'e}n},
	date-added = {2024-02-08 13:21:17 +0530},
	date-modified = {2024-02-08 13:21:17 +0530},
	eprint = {2305.11752},
	primaryclass = {cs.LG},
	title = {Marginalized Beam Search Algorithms for Hierarchical HMMs},
	year = {2023}}

@inproceedings{christodoulopoulos-etal-2010-two,
	address = {Cambridge, MA},
	author = {Christodoulopoulos, Christos and Goldwater, Sharon and Steedman, Mark},
	booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2024-02-08 12:29:56 +0530},
	date-modified = {2024-02-08 12:29:56 +0530},
	editor = {Li, Hang and M{\`a}rquez, Llu{\'\i}s},
	month = oct,
	pages = {575--584},
	publisher = {Association for Computational Linguistics},
	title = {Two Decades of Unsupervised {POS} Induction: How Far Have We Come?},
	url = {https://aclanthology.org/D10-1056},
	year = {2010},
	bdsk-url-1 = {https://aclanthology.org/D10-1056}}

@inproceedings{vauquois1968survey,
	author = {Vauquois, Bernard},
	booktitle = {Ifip congress (2)},
	date-added = {2023-11-01 11:44:19 +0530},
	date-modified = {2023-11-01 11:44:19 +0530},
	pages = {1114--1122},
	title = {A survey of formal grammars and algorithms for recognition and transformation in mechanical translation.},
	volume = {68},
	year = {1968}}

@article{pado-lapata-2007-dependency,
	author = {Pad{\'o}, Sebastian and Lapata, Mirella},
	date-added = {2023-09-19 05:44:12 +0530},
	date-modified = {2023-09-19 05:44:12 +0530},
	doi = {10.1162/coli.2007.33.2.161},
	journal = {Computational Linguistics},
	number = {2},
	pages = {161--199},
	title = {Dependency-Based Construction of Semantic Space Models},
	url = {https://aclanthology.org/J07-2002},
	volume = {33},
	year = {2007},
	bdsk-url-1 = {https://aclanthology.org/J07-2002},
	bdsk-url-2 = {https://doi.org/10.1162/coli.2007.33.2.161}}

@article{Harris:1954aa,
	added-at = {2020-05-20T16:56:27.000+0200},
	author = {Harris, Zellig},
	biburl = {https://www.bibsonomy.org/bibtex/252e7950c31610617170d71c320f2252e/ghagerer},
	date-added = {2023-05-21 08:08:19 +0530},
	date-modified = {2023-05-21 08:08:19 +0530},
	doi = {10.1007/978-94-009-8467-7_1},
	interhash = {a23596808b6273076e1259dedca16330},
	intrahash = {52e7950c31610617170d71c320f2252e},
	journal = {Word},
	keywords = {bag-of-words},
	number = {2-3},
	pages = {146--162},
	publisher = {Taylor \& Francis},
	timestamp = {2020-06-24T14:53:20.000+0200},
	title = {Distributional structure},
	url = {https://link.springer.com/chapter/10.1007/978-94-009-8467-7_1},
	volume = 10,
	year = 1954,
	bdsk-url-1 = {https://link.springer.com/chapter/10.1007/978-94-009-8467-7_1},
	bdsk-url-2 = {https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520},
	bdsk-url-3 = {https://doi.org/10.1007/978-94-009-8467-7_1}}

@article{bojanowski-etal-2017-enriching,
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	address = {Cambridge, MA},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	date-added = {2023-05-21 07:27:03 +0530},
	date-modified = {2023-05-21 07:27:03 +0530},
	doi = {10.1162/tacl_a_00051},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {135--146},
	publisher = {MIT Press},
	title = {Enriching Word Vectors with Subword Information},
	url = {https://aclanthology.org/Q17-1010},
	volume = {5},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/Q17-1010},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00051}}

@inproceedings{Schutze92,
	address = {Washington, DC, USA},
	author = {Sch\"{u}tze, H.},
	booktitle = {Proceedings of the 1992 ACM/IEEE Conference on Supercomputing},
	date-added = {2023-05-21 07:14:52 +0530},
	date-modified = {2023-05-21 07:15:31 +0530},
	isbn = {0818626305},
	location = {Minneapolis, Minnesota, USA},
	numpages = {10},
	pages = {787--796},
	publisher = {IEEE Computer Society Press},
	series = {Supercomputing '92},
	title = {Dimensions of Meaning},
	year = {1992}}

@webpage{graves2014generating,
	archiveprefix = {arXiv},
	author = {Alex Graves},
	date-added = {2023-03-31 10:52:35 +0530},
	date-modified = {2024-09-25 18:53:05 +0530},
	eprint = {1308.0850},
	primaryclass = {cs.NE},
	title = {Generating Sequences With Recurrent Neural Networks},
	url = {http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf},
	year = {2014},
	bdsk-url-1 = {http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf},
	bdsk-url-2 = {https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf}}

@inproceedings{gehring2017convolutional,
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
	booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	date-added = {2023-03-31 10:28:13 +0530},
	date-modified = {2023-03-31 10:28:13 +0530},
	pages = {1243--1252},
	title = {Convolutional sequence to sequence learning},
	year = {2017}}

@inproceedings{Vaswani2017,
	abstract = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2023-03-31 10:26:44 +0530},
	date-modified = {2025-01-22 21:08:47 +0530},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}}

@inproceedings{NalRecurrentTranslationModel2013,
	address = {Seattle, Washington, USA},
	author = {Kalchbrenner, Nal and Blunsom, Phil},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2023-03-30 19:14:21 +0530},
	date-modified = {2023-03-30 19:14:48 +0530},
	month = oct,
	pages = {1700--1709},
	publisher = {Association for Computational Linguistics},
	title = {Recurrent Continuous Translation Models},
	url = {https://aclanthology.org/D13-1176},
	year = {2013},
	bdsk-url-1 = {https://aclanthology.org/D13-1176}}

@inproceedings{sutskever_seq2Seq_2014,
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	address = {Cambridge, MA, USA},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
	date-added = {2023-03-30 19:08:16 +0530},
	date-modified = {2023-03-30 19:08:56 +0530},
	location = {Montreal, Canada},
	numpages = {9},
	pages = {3104--3112},
	publisher = {MIT Press},
	series = {NIPS'14},
	title = {Sequence to Sequence Learning with Neural Networks},
	year = {2014}}

@inproceedings{koehn-etal-2003-statistical,
	author = {Koehn, Philipp and Och, Franz J. and Marcu, Daniel},
	booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics},
	date-added = {2023-03-30 18:49:26 +0530},
	date-modified = {2023-03-30 18:49:26 +0530},
	pages = {127--133},
	title = {Statistical Phrase-Based Translation},
	url = {https://aclanthology.org/N03-1017},
	year = {2003},
	bdsk-url-1 = {https://aclanthology.org/N03-1017}}

@inproceedings{bahdanau2015neural,
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
	date-added = {2023-03-30 18:08:58 +0530},
	date-modified = {2023-11-14 22:02:35 +0530},
	title = {Neural machine translation by jointly learning to align and translate},
	year = {2015}}

@article{DBLP:journals/corr/abs-1003-1141,
	author = {Peter D. Turney and Patrick Pantel},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1003-1141.bib},
	date-added = {2023-03-21 07:00:23 +0530},
	date-modified = {2023-03-21 07:00:23 +0530},
	eprint = {1003.1141},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:20 +0200},
	title = {From Frequency to Meaning: Vector Space Models of Semantics},
	url = {http://arxiv.org/abs/1003.1141},
	volume = {abs/1003.1141},
	year = {2010},
	bdsk-url-1 = {http://arxiv.org/abs/1003.1141}}

@article{DBLP:journals/corr/abs-1802-05365,
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
	date-added = {2023-03-21 06:57:07 +0530},
	date-modified = {2025-01-22 21:15:37 +0530},
	eprint = {1802.05365},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	volume = {abs/1802.05365},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1802.05365}}

@book{Dash2018,
	abstract = {Defining the characteristic features of a corpus, in general, has been an issue of great debate for decades. Due to diversities involved in the types of text used for corpus generation, identification of features has been an area of continuous change and modification. Taking this challenge, in this chapter, we have taken the effort to propose and prescribe some of the basic features which are general to all kinds of language corpus. We have proposed features like `Quantity' that refers to the amount of language data stored in a corpus; `Quality' that hints for authenticity of texts used in a corpus; `Representation' that refers to the area of coverage of texts of a language; `Simplicity' that refers to the mode of composition of corpus in digital form; `Equality' that refers to even distribution of texts from all domains of language use; `Retrievability' that argues for accessing language data in an easy and simplified manner; `Verifiability' that refers to making texts available for all kinds of manual and machine verification of originality of texts; `Augmentation' that argues for regular increase of corpus size with addition of new text samples; `Documentation' that refers to recording the text types and sources of texts in full details for linguistic and extralinguistic (e.g., legal, etc.) reasons; and `Management' that suggests for storing, classifying, and processing data in a systematic manner for eventual usage by one and all in all domains of human knowledge where language is indispensable.},
	author = {Dash, Niladri Sekhar and Arulmozi, S.},
	booktitle = {History, Features, and Typology of Language Corpora},
	date = {2018},
	doi = {10.1007/978-981-10-7458-5_2},
	isbn = {978-981-10-7458-5},
	location = {Singapore},
	pages = {17--34},
	publisher = {Springer Singapore},
	title = {{Features of a Corpus}},
	bdsk-url-1 = {https://doi.org/10.1007/978-981-10-7458-5_2}}

@book{Loper:2002:NNL:1118108.1118117,
	acmid = {1118117},
	address = {Stroudsburg, PA, USA},
	author = {Loper, Edward and Bird, Steven},
	booktitle = {Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1},
	date-modified = {2024-11-12 04:54:16 +0530},
	doi = {10.3115/1118108.1118117},
	location = {Philadelphia, Pennsylvania},
	numpages = {8},
	pages = {63--70},
	publisher = {Association for Computational Linguistics},
	series = {ETMTNLP '02},
	title = {{NLTK: The Natural Language Toolkit}},
	url = {https://doi.org/10.3115/1118108.1118117},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.3115/1118108.1118117}}

@book{Manning2009,
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schutze, Hinrich},
	chapter = {6},
	pages = {109-133},
	publisher = {Cambridge UP},
	title = {{An Introduction to Information Retrieval}},
	year = {2009}}

@book{Manning1999,
	author = {Manning et al},
	date = {1999},
	groups = {ramaseshan:6},
	isbn = {9780262133609},
	lccn = {99021137},
	publisher = {MIT Press},
	series = {Mit Press},
	title = {Foundations of statistical natural language processing},
	url = {https://books.google.co.in/books?id=YiFDxbEX3SUC},
	year = {1999},
	bdsk-url-1 = {https://books.google.co.in/books?id=YiFDxbEX3SUC}}

@inproceedings{Mnih:2008:SHD:2981780.2981915,
	acmid = {2981915},
	address = {USA},
	author = {Mnih, Andriy and Hinton, Geoffrey},
	booktitle = {Proceedings of the 21st International Conference on Neural Information Processing Systems},
	isbn = {978-1-6056-0-949-2},
	location = {Vancouver, British Columbia, Canada},
	numpages = {8},
	pages = {1081--1088},
	publisher = {Curran Associates Inc.},
	series = {NIPS'08},
	title = {A Scalable Hierarchical Distributed Language Model},
	url = {http://dl.acm.org/citation.cfm?id=2981780.2981915},
	year = {2008},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=2981780.2981915}}

@inproceedings{Mikolov:2013:DRW:2999792.2999959,
	acmid = {2999959},
	address = {USA},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
	location = {Lake Tahoe, Nevada},
	numpages = {9},
	pages = {3111--3119},
	publisher = {Curran Associates Inc.},
	series = {NIPS'13},
	title = {Distributed Representations of Words and Phrases and Their Compositionality},
	url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
	year = {2013},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=2999792.2999959}}

@article{DBLP:journals/corr/Dyer14,
	archiveprefix = {arXiv},
	author = {Chris Dyer},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/Dyer14},
	eprint = {1410.8251},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:48:11 +0200},
	title = {{Notes on Noise Contrastive Estimation and Negative Sampling}},
	url = {http://arxiv.org/abs/1410.8251},
	volume = {abs/1410.8251},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1410.8251}}

@article{Bengio:2003:NPL:944919.944966,
	acmid = {944966},
	author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
	issn = {1532-4435},
	issue_date = {3/1/2003},
	journal = {Journal of Machine Learning Research},
	month = mar,
	numpages = {19},
	pages = {1137--1155},
	publisher = {JMLR.org},
	title = {{A Neural Probabilistic Language Model}},
	url = {http://dl.acm.org/citation.cfm?id=944919.944966},
	volume = {3},
	year = {2003},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=944919.944966}}

@webpage{journals/corr/abs-1301-3781,
	added-at = {2013-10-23T23:02:12.000+0200},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	biburl = {https://www.bibsonomy.org/bibtex/29665b85e8756834ac29fcbd2c6ad0837/wool},
	date-modified = {2024-09-24 09:00:51 +0530},
	ee = {http://arxiv.org/abs/1301.3781},
	interhash = {e92df552b17e9f952226a893b84ad739},
	intrahash = {9665b85e8756834ac29fcbd2c6ad0837},
	journal = {CoRR},
	keywords = {nlp},
	timestamp = {2013-10-23T23:02:12.000+0200},
	title = {{Efficient Estimation of Word Representations in Vector Space}},
	url = {http://arxiv.org/abs/1301.3781},
	volume = {abs/1301.3781},
	year = {2013},
	bdsk-url-1 = {http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781},
	bdsk-url-2 = {http://arxiv.org/abs/1301.3781}}

@webpage{DBLP:journals/corr/Rong14,
	archiveprefix = {arXiv},
	author = {Xin Rong},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/Rong14},
	date-modified = {2024-09-16 06:18:24 +0530},
	eprint = {1411.2738},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
	title = {{word2vec Parameter Learning Explained}},
	url = {https://sites.socsci.uci.edu/~lpearl/courses/readings/Rong2014_Word2Vec.pdf},
	volume = {abs/1411.2738},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1411.2738},
	bdsk-url-2 = {https://sites.socsci.uci.edu/~lpearl/courses/readings/Rong2014_Word2Vec.pdf}}

@article{scikit-learn,
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal = {Journal of Machine Learning Research},
	pages = {2825--2830},
	title = {{Scikit-learn: Machine Learning in {P}ython}},
	volume = {12},
	year = {2011}}

@article{Brown:1992:CNG:176313.176316,
	acmid = {176316},
	address = {Cambridge, MA, USA},
	author = {Brown, Peter F. and deSouza, Peter V. and Mercer, Robert L. and Pietra, Vincent J. Della and Lai, Jenifer C.},
	issn = {0891-2017},
	issue_date = {December 1992},
	journal = {Comput. Linguist.},
	month = dec,
	number = {4},
	numpages = {13},
	pages = {467--479},
	publisher = {MIT Press},
	title = {{Class-based N-gram Models of Natural Language}},
	url = {http://dl.acm.org/citation.cfm?id=176313.176316},
	volume = {18},
	year = {1992},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=176313.176316}}

@article{Smith20099,
	abstract = {Even within the early sensory areas, the majority of the input to any given cortical neuron comes from other cortical neurons. To extend our knowledge of the contextual information that is transmitted by such lateral and feedback connections, we investigated how visually nonstimulated regions in primary visual cortex (V1) and visual area V2 are influenced by the surrounding context. We used functional magnetic resonance imaging (fMRI) and pattern-classification methods to show that the cortical representation of a nonstimulated quarter-field carries information that can discriminate the surrounding visual context. We show further that the activity patterns in these regions are significantly related to those observed with feed-forward stimulation and that these effects are driven primarily by V1. These results thus demonstrate that visual context strongly influences early visual areas even in the absence of differential feed-forward thalamic stimulation.},
	author = {Smith, Fraser W. and Muckli, Lars},
	doi = {10.1073/pnas.1000233107},
	eprint = {https://www.pnas.org/content/107/46/20099.full.pdf},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	number = {46},
	pages = {20099--20103},
	publisher = {National Academy of Sciences},
	title = {Nonstimulated early visual areas carry information about surrounding context},
	url = {https://www.pnas.org/content/107/46/20099},
	volume = {107},
	year = {2010},
	bdsk-url-1 = {https://www.pnas.org/content/107/46/20099},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1000233107}}

@book{smith:2011:synthesis,
	author = {Noah A. Smith},
	month = {May},
	publisher = {Morgan and Claypool},
	series = {Synthesis Lectures on Human Language Technologies},
	title = {{Linguistic Structure Prediction}},
	year = {2011}}

@webpage{Hochreiter:1997:LSM:1246443.1246450,
	acmid = {1246450},
	address = {Cambridge, MA, USA},
	author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
	date-modified = {2024-09-29 18:30:13 +0530},
	doi = {10.1162/neco.1997.9.8.1735},
	issn = {0899-7667},
	issue_date = {November 15, 1997},
	journal = {{Neural Comput.}},
	month = nov,
	number = {8},
	numpages = {46},
	pages = {1735--1780},
	publisher = {MIT Press},
	title = {{Long Short-Term Memory}},
	url = {https://www.bioinf.jku.at/publications/older/2604.pdf},
	volume = {9},
	year = {1997},
	bdsk-url-1 = {https://www.bioinf.jku.at/publications/older/2604.pdf},
	bdsk-url-2 = {https://doi.org/10.1162/neco.1997.9.8.1735}}

@article{Musashi:13123221,
	abstract = {{A plastic nervous system requires the ability not only to acquire and store but also to forget. Here, we report that musashi (msi-1) is necessary for time-dependent memory loss in C. elegans. Tissue-specific rescue demonstrates that MSI-1 function is necessary in the AVA interneuron. Using RNA-binding protein immunoprecipitation (IP), we found that MSI-1 binds to mRNAs of three subunits of the Arp2/3 actin branching regulator complex in vivo and downregulates ARX-1, ARX-2, and ARX-3 translation upon associative learning. The role of msi-1 in forgetting is also reflected by the persistence of learning-induced GLR-1 synaptic size increase in msi-1 mutants. We demonstrate that memory length is regulated cooperatively through the activation of adducin (add-1) and by the inhibitory effect of msi-1. Thus, a GLR-1/MSI-1/Arp2/3 pathway induces forgetting and represents a novel mechanism of memory decay by linking translational control to the structure of the actin cytoskeleton in neurons. Copyright {\copyright} 2014 Elsevier Inc. All rights reserved.}},
	author = {Hadziselimovic, Nils and Vukojevic, Vanja and Peter, Fabian and Milnik, Annette and Fastenrath, Matthias and Fenyves, Bank Gabor G. and Hieber, Petra and Demougin, Philippe and Vogler, Christian and de Quervain, Dominique J-F J. and Papassotiropoulos, Andreas and Stetak, Attila},
	citeulike-article-id = {13123221},
	citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/24630719},
	citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=24630719},
	day = {13},
	issn = {1097-4172},
	journal = {Cell},
	keywords = {c\_elegans, memory, msi1, rna-protein\_interaction},
	month = mar,
	number = {6},
	pages = {1153--1166},
	pmid = {24630719},
	posted-at = {2014-04-02 01:07:26},
	title = {{Forgetting Is Regulated via Musashi-Mediated Translational Control of the Arp2/3 Complex.}},
	url = {http://view.ncbi.nlm.nih.gov/pubmed/24630719},
	volume = {156},
	year = {2014},
	bdsk-url-1 = {http://view.ncbi.nlm.nih.gov/pubmed/24630719}}

@article{Brown:1993:MSM:972470.972474,
	acmid = {972474},
	address = {Cambridge, MA, USA},
	author = {Brown, Peter F. and Pietra, Vincent J. Della and Pietra, Stephen A. Della and Mercer, Robert L.},
	issn = {0891-2017},
	issue_date = {June 1993},
	journal = {Comput. Linguist.},
	month = jun,
	number = {2},
	numpages = {49},
	pages = {263--311},
	publisher = {MIT Press},
	title = {The Mathematics of Statistical Machine Translation: Parameter Estimation},
	url = {http://dl.acm.org/citation.cfm?id=972470.972474},
	volume = {19},
	year = {1993},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=972470.972474}}

@article{DBLP:journals/corr/LuongPM15,
	archiveprefix = {arXiv},
	author = {Minh{-}Thang Luong and Hieu Pham and Christopher D. Manning},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/LuongPM15},
	eprint = {1508.04025},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:14 +0200},
	title = {Effective Approaches to Attention-based Neural Machine Translation},
	url = {http://arxiv.org/abs/1508.04025},
	volume = {abs/1508.04025},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1508.04025}}

@article{bahdanauNMT2014,
	author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	date-modified = {2023-03-30 18:03:23 +0530},
	journal = {arXiv},
	language = {English (US)},
	title = {Neural machine translation by jointly learning to align and translate},
	year = {2014}}

@webpage{Deerwester90indexingby,
	author = {Scott Deerwester and Susan T. Dumais and George W. Furnas and Thomas K. Landauer and Richard Harshman},
	date-modified = {2024-09-04 19:26:51 +0530},
	journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
	number = {6},
	pages = {391--407},
	title = {Indexing by latent semantic analysis},
	url = {https://psychology.uwo.ca/faculty/harshman/latentsa.pdf},
	volume = {41},
	year = {1990},
	bdsk-url-1 = {https://psychology.uwo.ca/faculty/harshman/latentsa.pdf}}

@book{Jurafsky:2000:SLP:555733,
	address = {Upper Saddle River, NJ, USA},
	author = {Jurafsky, Daniel and Martin, James H.},
	edition = {1st},
	isbn = {0130950696},
	publisher = {Prentice Hall PTR},
	title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	year = {2000}}

@inproceedings{morin2005hierarchical,
	author = {Morin, Frederic and Bengio, Yoshua},
	booktitle = {Aistats},
	organization = {Citeseer},
	pages = {246--252},
	title = {Hierarchical probabilistic neural network language model.},
	volume = {5},
	year = {2005}}

@inproceedings{le2014distributed,
	author = {Le, Quoc and Mikolov, Tomas},
	booktitle = {International conference on machine learning},
	keywords = {Sentence, paragraph learning},
	pages = {1188--1196},
	title = {Distributed representations of sentences and documents},
	url = {http://proceedings.mlr.press/v32/le14.pdf},
	year = {2014},
	bdsk-url-1 = {http://proceedings.mlr.press/v32/le14.pdf}}

@webpage{Goodfellow-et-al-2016,
	author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	date-modified = {2024-10-12 18:27:24 +0530},
	note = {\url{http://www.deeplearningbook.org}},
	publisher = {MIT Press},
	title = {Deep Learning},
	url = {https://www.deeplearningbook.org},
	year = {2016},
	bdsk-url-1 = {https://www.deeplearningbook.org}}

@article{journals/corr/KarpathyJL15,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Karpathy, Andrej and Johnson, Justin and Li, Fei-Fei},
	biburl = {https://www.bibsonomy.org/bibtex/2b92f50d756d7099a0cf63e69d1ec2699/dblp},
	ee = {http://arxiv.org/abs/1506.02078},
	interhash = {443385d2fd54fb96392d954ecba32cca},
	intrahash = {b92f50d756d7099a0cf63e69d1ec2699},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T13:03:03.000+0200},
	title = {Visualizing and Understanding Recurrent Networks.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1506.html#KarpathyJL15},
	volume = {abs/1506.02078},
	year = {2015},
	bdsk-url-1 = {http://dblp.uni-trier.de/db/journals/corr/corr1506.html#KarpathyJL15}}

@inproceedings{Jozefowicz:2015:EER:3045118.3045367,
	acmid = {3045367},
	author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
	booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
	location = {Lille, France},
	numpages = {9},
	pages = {2342--2350},
	publisher = {JMLR.org},
	series = {ICML'15},
	title = {An Empirical Exploration of Recurrent Network Architectures},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045367},
	year = {2015},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=3045118.3045367}}

@article{Och:2004:ATA:1105587.1105589,
	acmid = {1105589},
	address = {Cambridge, MA, USA},
	author = {Och, Franz Josef and Ney, Hermann},
	doi = {10.1162/0891201042544884},
	issn = {0891-2017},
	issue_date = {December 2004},
	journal = {Computational Linguistics},
	month = dec,
	number = {4},
	numpages = {33},
	pages = {417--449},
	publisher = {MIT Press},
	title = {The Alignment Template Approach to Statistical Machine Translation},
	url = {http://dx.doi.org/10.1162/0891201042544884},
	volume = {30},
	year = {2004},
	bdsk-url-1 = {http://dx.doi.org/10.1162/0891201042544884}}

@inproceedings{papineni-etal-2002-bleu,
	address = {Philadelphia, Pennsylvania, USA},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	booktitle = {Proceedings of 40th Annual Meeting of the Association for Computational Linguistics},
	doi = {10.3115/1073083.1073135},
	month = jul,
	pages = {311--318},
	publisher = {Association for Computational Linguistics},
	title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
	url = {https://www.aclweb.org/anthology/P02-1040},
	year = {2002},
	bdsk-url-1 = {https://www.aclweb.org/anthology/P02-1040},
	bdsk-url-2 = {https://doi.org/10.3115/1073083.1073135}}

@article{DBLP:journals/corr/ChoMBB14,
	archiveprefix = {arXiv},
	author = {KyungHyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/ChoMBB14},
	eprint = {1409.1259},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
	title = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
	url = {http://arxiv.org/abs/1409.1259},
	volume = {abs/1409.1259},
	year = {2014},
	bdsk-url-1 = {http://arxiv.org/abs/1409.1259}}

@article{DBLP:journals/corr/HoriH17,
	archiveprefix = {arXiv},
	author = {Chiori Hori and Takaaki Hori},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/HoriH17},
	eprint = {1706.07440},
	eprinttype = {arxiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:45 +0200},
	title = {End-to-end Conversation Modeling Track in {DSTC6}},
	volume = {abs/1706.07440},
	year = {2017}}

@article{DBLP:journals/corr/Goldberg15c,
	archiveprefix = {arXiv},
	author = {Yoav Goldberg},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/Goldberg15c},
	eprint = {1510.00726},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
	title = {A Primer on Neural Network Models for Natural Language Processing},
	url = {http://arxiv.org/abs/1510.00726},
	volume = {abs/1510.00726},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1510.00726}}

@book{indurkhya2010handbook,
	author = {Indurkhya, Nitin and Damerau, Fred J},
	publisher = {Chapman and Hall/CRC},
	title = {Handbook of natural language processing},
	year = {2010}}

@book{perkins2014python,
	author = {Perkins, Jacob},
	publisher = {Packt Publishing Ltd},
	title = {Python 3 text processing with NLTK 3 cookbook},
	year = {2014}}

@book{pacsca2003open,
	author = {Pa{\c{s}}ca, Marius},
	publisher = {MIT Press},
	title = {Open-domain question answering from large text collections},
	year = {2003}}

@article{DanqiChen2017,
	abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
	author = {Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
	date = {2017-03-31},
	eprint = {1704.00051v2},
	eprintclass = {cs.CL},
	eprinttype = {arXiv},
	file = {online:http\://arxiv.org/pdf/1704.00051v2:PDF},
	journal = {CL},
	keywords = {cs.CL},
	title = {Reading Wikipedia to Answer Open-Domain Questions},
	year = {2017}}

@article{Shang2015,
	abstract = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
	author = {Lifeng Shang and Zhengdong Lu and Hang Li},
	date = {2015-03-09},
	eprint = {1503.02364v2},
	eprintclass = {cs.CL},
	eprinttype = {arXiv},
	file = {online:http\://arxiv.org/pdf/1503.02364v2:PDF},
	journal = {CL},
	keywords = {cs.CL, cs.AI, cs.NE},
	title = {Neural Responding Machine for Short-Text Conversation},
	year = {2015}}

@article{Zongcheng2014,
	abstract = {Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather "intelligently", when combined with a huge repository of conversation data from social media.},
	author = {Zongcheng Ji and Zhengdong Lu and Hang Li},
	date = {2014-08-29},
	eprint = {1408.6988v1},
	eprintclass = {cs.IR},
	eprinttype = {arXiv},
	file = {online:http\://arxiv.org/pdf/1408.6988v1:PDF},
	journal = {arxiv},
	keywords = {cs.IR, cs.CL},
	title = {An Information Retrieval Approach to Short Text Conversation},
	year = {2014}}

@inproceedings{rajpurkar-etal-2016-squad,
	address = {Austin, Texas},
	annote = {SQUAD},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	date-modified = {2024-02-09 06:56:50 +0530},
	doi = {10.18653/v1/D16-1264},
	month = nov,
	pages = {2383--2392},
	publisher = {Association for Computational Linguistics},
	title = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {https://www.aclweb.org/anthology/D16-1264},
	year = {2016},
	bdsk-url-1 = {https://www.aclweb.org/anthology/D16-1264},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D16-1264}}

@article{lund_producing_1996,
	abstract = {A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).},
	added-at = {2018-11-04T17:02:36.000+0100},
	author = {Lund, Kevin and Burgess, Curt},
	biburl = {https://www.bibsonomy.org/bibtex/234275296af6da19ad0e7061b4fac28dc/lepsky},
	doi = {10.3758/BF03204766},
	interhash = {77e182745556c0a24e0e67a22652a66e},
	intrahash = {34275296af6da19ad0e7061b4fac28dc},
	issn = {0743-3808, 1532-5970},
	journal = {Behavior Research Methods, Instruments, \& Computers},
	keywords = {semantik},
	language = {en},
	number = {2},
	pages = {203--208},
	timestamp = {2018-11-04T17:02:36.000+0100},
	title = {Producing high-dimensional semantic spaces from lexical co-occurrence},
	url = {http://link.springer.com/article/10.3758/BF03204766},
	urldate = {2015-09-09},
	volume = {28},
	year = {1996},
	bdsk-url-1 = {http://link.springer.com/article/10.3758/BF03204766},
	bdsk-url-2 = {https://doi.org/10.3758/BF03204766}}

@article{DBLP:journals/corr/AroraLLMR16,
	archiveprefix = {arXiv},
	author = {Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/AroraLLMR16},
	eprint = {1601.03764},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:48:57 +0200},
	title = {Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
	url = {http://arxiv.org/abs/1601.03764},
	volume = {abs/1601.03764},
	year = {2016},
	bdsk-url-1 = {http://arxiv.org/abs/1601.03764}}

@inproceedings{pennington2014glove,
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
	pages = {1532--1543},
	title = {Glove: Global vectors for word representation},
	year = {2014}}

@article{DBLP:journals/corr/LebretL13,
	abstract = {Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.},
	archiveprefix = {arXiv},
	author = {R{\'{e}}mi Lebret and Ronan Lebret},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/LebretL13},
	date-modified = {2025-01-22 21:37:47 +0530},
	eprint = {1312.5542},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:48:28 +0200},
	title = {Word Emdeddings through Hellinger {PCA}},
	url = {http://arxiv.org/abs/1312.5542},
	volume = {abs/1312.5542},
	year = {2013},
	bdsk-url-1 = {http://arxiv.org/abs/1312.5542}}

@article{Weizenbaum:1966:ECP:365153.365168,
	abstract = {ELIZA is a program operating within the MAC time-sharing system at MIT which makes certain kinds of natural language conversation between man and computer possible. Input sen- tences are analyzed on the basis of decomposition rules which
are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical prob- lems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA "scripts". A discussion of some psychologi- cal issues relevant to the ELIZA approach as well as of future
developments concludes the paper.},
	acmid = {365168},
	address = {New York, NY, USA},
	author = {Weizenbaum, Joseph},
	date-modified = {2025-01-22 21:19:19 +0530},
	doi = {10.1145/365153.365168},
	issn = {0001-0782},
	issue_date = {Jan. 1966},
	journal = {Commun. ACM},
	month = jan,
	number = {1},
	numpages = {10},
	pages = {36--45},
	publisher = {ACM},
	title = {ELIZA\&Mdash;a Computer Program for the Study of Natural Language Communication Between Man and Machine},
	url = {http://doi.acm.org/10.1145/365153.365168},
	volume = {9},
	year = {1966},
	bdsk-url-1 = {http://doi.acm.org/10.1145/365153.365168},
	bdsk-url-2 = {https://doi.org/10.1145/365153.365168}}

@inproceedings{tsvetkov-etal-2015-evaluation,
	abstract = {Unsupervisedly learned word vectors have
proven to provide exceptionally effective
features in many NLP tasks. Most common
intrinsic evaluations of vector quality mea-
sure correlation with similarity judgments.
However, these often correlate poorly with
how well the learned representations per-
form as features in downstream evaluation
tasks. We present QVEC---a computation-
ally inexpensive intrinsic evaluation mea-
sure of the quality of word embeddings
based on alignment to a matrix of features
extracted from manually crafted lexical
resources---that obtains strong correlation
with performance of the vectors in a battery
of downstream semantic evaluation tasks.1},
	address = {Lisbon, Portugal},
	author = {Tsvetkov, Yulia and Faruqui, Manaal and Ling, Wang and Lample, Guillaume and Dyer, Chris},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	date-modified = {2025-01-22 21:26:36 +0530},
	doi = {10.18653/v1/D15-1243},
	month = sep,
	pages = {2049--2054},
	publisher = {Association for Computational Linguistics},
	title = {Evaluation of Word Vector Representations by Subspace Alignment},
	url = {https://www.aclweb.org/anthology/D15-1243},
	year = {2015},
	bdsk-url-1 = {https://www.aclweb.org/anthology/D15-1243},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D15-1243}}

@comment{BibDesk Smart Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Conversation model</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Conversation model</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Deep Learning</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Deep Learning</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>HMM</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Hidden Markov</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>HMM</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Information Retrieval</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Information Retrieval</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Knowledge graph</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Knowledge graph</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Language model</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Recurrent Neural</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>sequence to sequence</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>Language model</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Machine Comprehension</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>comprehend</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>Machine Comprehension</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Bayesian</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Machine Learning Techniques</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>translation</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Machine Translation</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>multimodal</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Multimodal</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>BibTeX Type</string>
				<key>value</key>
				<string>book</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>NLP</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>PCA</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>PCA</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Python</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Python</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>representation learning</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>representation learning</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>RNN</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>RNN</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>sentiment</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>sentiment</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>transformer</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>GPT</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>Transformer</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>word embedding</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>word vectors</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>word2vec</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>CBOW</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>skip-gram</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>Glove</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>HAL</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>COALS</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>dense vectors</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>embeddings</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>semantic space</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>BPE</string>
				<key>version</key>
				<string>1</string>
			</dict>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Any Field</string>
				<key>value</key>
				<string>latent semantic</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>1</integer>
		<key>group name</key>
		<string>word embedding</string>
	</dict>
</array>
</plist>
}}
