\begin{frame}[allowframebreaks]{Introduction}
\begin{itemize}
    \item 	LLMs have transformed artificial intelligence, extending beyond traditional NLP.
	\item They learn knowledge and reasoning from data, unlike formal logic systems that struggled with real-world complexity.
	\item LLMs’ ability to acquire knowledge opens up a new avenue in reasoning.
	\item They may lead to verifiability in many tasks, such as legal analysis, scientific discovery, $\cdots$.
	\item Traditional reasoning in AI involves applying structured rules to derive conclusions, while LLM-based reasoning integrates natural language understanding and enables multi-step deduction and abstraction.
	\item Algorithmic reasoning may lead to multi-step thought processes, which may enhance the clarity and trustworthiness of LLM outcomes.
    \framebreak
    \item Logical, common-sense, and mathematical reasoning are crucial for AI systems
    \item Provide analytical skills. Formal and symbolic logic-based reasoning will be distinguished from heuristic approaches like Chain-of-Thought prompting.
    \item The attention mechanism may facilitate coherent thought generation.
    \item Empirical evidence suggests a correlation between LLM scale and reasoning abilities.
    \item Parameters and training data (AKA scale) is critical for unlocking reasoning potential. LLM Performance is directly proportional to scale (demonstrated - ELMO$ \rightarrow$ transformer)
    \item Deep dive into into the theoretical framework is needed to improve the model design, training, and prompting strategies.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Deductive Reasoning (Top-Down Logic)}

\textbf{What it is:} Starting with a general rule and applying it to a specific situation to reach a certain conclusion.

\bigskip % Add some vertical space

\begin{block}{Simple Example}
  \begin{itemize}
    \item \textbf{General Rule:} All dogs bark.
    \item \textbf{Specific Case:} Fido is a dog.
    \item \textbf{Deductive Conclusion:} Therefore, Fido barks.
  \end{itemize}
\end{block}

\bigskip

Generally good at simple deductions, but struggle with complex, multi-step logic or strict coherence in long arguments.
\end{frame}

% Frame 2: Inductive Reasoning
\begin{frame}[fragile]
\frametitle{Inductive Reasoning (Bottom-Up Logic)}

Forming a general rule or conclusion from specific examples or observations. This conclusion is likely, but not necessarily true.

\bigskip

\begin{block}{Simple Example}
  \begin{itemize}
    \item \textbf{Observation 1:} I am a saggitarian. I spill coffee \verb|;)|
    \item \textbf{Observation 2:} My uncle is a saggitarian and he spills coffee
    \item \textbf{Inductive Conclusion:} Therefore, many saggitarians probably spill coffee.
  \end{itemize}
\end{block}

\bigskip

LLMs excel at generalizing from observed patterns but might not invent entirely novel hypotheses far beyond their training data.
\end{frame}

% Frame 3: Abductive Reasoning
\begin{frame}
\frametitle{Abductive Reasoning (Making the Best Guess)}

Observing an outcome and guessing the most likely cause based on incomplete information, like a detective making an educated guess.

\bigskip

\begin{block}{Simple Example}
  \begin{itemize}
    \item \textbf{Observation:} The street outside is wet.
    \item \textbf{Possible Explanations:} It rained, a street cleaner went by, someone spilled water.
    \item \textbf{Abductive Conclusion (Best Guess):} The most likely explanation is that it rained (based on common occurrences).
  \end{itemize}
\end{block}

\bigskip

 Suggest plausible explanations based on data correlations, but lack true understanding of causality and context, limiting reliability in complex situations.
\end{frame}

% Frame 4: Analogical Reasoning
\begin{frame}
\frametitle{Analogical Reasoning (Finding Similarities)}

Comparing similar things or situations to learn, infer, or explain something about one of them.

\bigskip

\begin{block}{Simple Example}
  Just like a \texttt{seed} needs soil and water to grow into a \textit{plant}, a \textit{child} needs care and education to grow into a capable \textit{adult}.
\end{block}

\bigskip

Capable of generating basic analogies based on linguistic similarity, they may miss deeper, conceptual parallels due to a lack of real-world understanding of the underlying relationships.
\end{frame}


\begin{frame}{Common Sense  Reasoning}
	LLMs demonstrate common-sense knowledge through their pretraining data and techniques like Chain-of-Thought prompting.
	External knowledge bases can enhance LLMs’ performance on common-sense tasks by providing context.
	LLM performance on common-sense tasks is assessed using benchmarks like CommonsenseQA, StrategyQA, HellaSWAG, PIQA, Social IQA, and OpenBookQA.
	LLMs struggle with common-sense reasoning, showing less improvement than logical or mathematical tasks, especially in smaller models.
	LLMs’ knowledge for common-sense answers is often unreliable, potentially incorrect, and misleading.
	LLMs exhibit significant performance variations based on cultural context, highlighting potential biases in their understanding of the world.

\end{frame}

\begin{frame}{Mathematical Reasoning}
	•	Mathematical thought is a critical capability for artificial intelligence.
	•	It involves the ability to solve mathematical problems and engage in theorem proving.
	•	Large Language Models have shown remarkable potential in handling various mathematical tasks.
	•	These tasks include algebraic manipulation, solving numerical problems, and even contributing to theorem proving.
	•	Research in this area focuses on both solving mathematical problems and theorem proving.
	•	The goal of theorem proving is to verify mathematical statements using formal logic.
	•	Techniques like Chain-of-Thought prompting have been effective in improving the performance of LLMs on mathematical problems.
	•	These techniques encourage LLMs to break down complex problems into smaller, more manageable steps.
	•	The ability of LLMs to perform mathematical thought is assessed using a variety of specialized benchmarks.
	•	These benchmarks include datasets such as GSM8K and MATH.
	•	GSM8K contains grade-school level math word problems, while MATH includes problems from high school mathematics competitions.
	•	Other benchmarks like JEEBench and MATH 401 further test the capabilities of these models on more advanced mathematical problems.
	•	The development and use of these benchmarks reflect ongoing efforts to rigorously evaluate and enhance the mathematical problem-solving abilities of LLMs.
	•	A significant challenge in evaluating mathematical thought in LLMs is distinguishing between genuine problem-solving and recalling solutions or patterns seen during training.
	•	Studies show that while LLMs can achieve high accuracy on certain mathematical benchmarks, their performance drops when faced with slight variations to the problems.
	•	This suggests a potential over-reliance on memorization rather than a deep understanding of mathematical concepts.
	•	LLMs can be easily distracted by irrelevant information in mathematical word problems, indicating a lack of ability to discern essential details from extraneous ones.
	•	The ability of LLMs to handle complex, multi-step mathematical reasoning and maintain coherence over longer problem-solving processes remains a limitation.
	•	In theorem proving, LLMs can sometimes generate incorrect steps or logical fallacies, indicating that ensuring mathematical rigor is an ongoing challenge.
	•	While LLMs have demonstrated impressive progress in mathematical thought, the extent to which this reflects true understanding and the ability to reason from first principles is still a subject of investigation.
	•	Their sensitivity to minor changes in problem statements and susceptibility to being misled by irrelevant information suggest that their current capabilities might not yet equate to genuine mathematical comprehension.
	•	The occurrence of errors in complex reasoning and theorem proving underscores the need for further research to ensure the reliability and accuracy of LLMs in mathematical applications.

\end{frame}